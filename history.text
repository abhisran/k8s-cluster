free -h
sudo swapoff -a
free -h
sudo vi /etc/fstab 
sudo rm /swap.img 
sudo apt update 
sudo apt upgrade -y
sudo vim /etc/ssh/sshd_config
ip a
sudo shutdown now
sudo vim /etc/netplan/00-installer-config.yaml 
ip a
ping google.com
cat /etc/resolv.conf 
sudo netplan apply
sudo reboot now
sudo vim /etc/netplan/00-installer-config.yaml 
sudo netplan apply
ip a
cat /etc/netplan/00-installer-config.yaml 
sudo cat /etc/netplan/00-installer-config.yaml
ping google.com
sudo cat /etc/netplan/00-installer-config.yaml
sudo vim ./etc/netplan/00-installer-config.yaml
sudo vim /etc/netplan/00-installer-config.yaml
sudo netplan apply
sudo netplan try && netplan apply
sudo netplan try && sudo netplan apply
ip a
ping google.com
sudo vim /etc/resolv.conf 
cat /etc/resolv.conf 
sudo cat /etc/netplan/00-installer-config.yaml 
ping google.com
sudo vim /etc/netplan/00-installer-config.yaml 
sudo netplan try && sudo netplan apply
ping google.com
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.ipv4.ip_forward = 1
EOF
sudo sysctl --system
sysctl net.ipv4.ip_forward
# Add Docker's official GPG key:
sudo apt-get update
sudo apt-get install ca-certificates curl
sudo install -m 0755 -d /etc/apt/keyrings
sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
sudo chmod a+r /etc/apt/keyrings/docker.asc
# Add the repository to Apt sources:
echo   "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \
  $(. /etc/os-release && echo "$VERSION_CODENAME") stable" |   sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
sudo apt-get update && sudo apt-get install containerd.io && systemctl enable --now containerd
sudo systemctl status containerd
wget https://github.com/containernetworking/plugins/releases/download/v1.4.0/cni-plugins-linux-amd64-v1.4.0.tgz
mkdir -p /opt/cni/bin
tar Cxzvf /opt/cni/bin cni-plugins-linux-amd64-v1.4.0.tgz
sudo mkdir -p /opt/cni/bin
tar Cxzvf /opt/cni/bin cni-plugins-linux-amd64-v1.4.0.tgz
sudo tar Cxzvf /opt/cni/bin cni-plugins-linux-amd64-v1.4.0.tgz
sudo -i
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
kubectl get node
sudo cat /etc/netplan/00-installer-config.yaml 
sudo kubeadm token create --print-join-command
kubectl get node
sudo shutdown now
pwd
ls -l
mkdir vscode
cd vscode/
kubectl get node
kubectl delete node worker-node
sudo kubeadm token create --print-join-command
kubectl get node
kubectl get pod -n kube-system
free -h
alias
alias k=kubectl
k get nodes -o wide
k get pods -n kube-system
k run nginxpod --image=nginx --port 80
k get pods
k describe pod nginxpod
k get pods
k get svc
k exec -it nginxpod -- /bin/sh
k logs nginxpod
k delete nginxpod
k delete pod nginxpod
k get nodes
k config view
k config current-context
k config get-context kubernetes-admin@kubernetes
k config get-contexts kubernetes-admin@kubernetes
k config get-contexts
k cluster-info
k cluster-info dump
ls
mv cni-plugins-linux-amd64-v1.4.0.tgz /tmp
ls
cd vscode/
mkdir cluster-administration
k cluster-info dump > cluster-dump
k get node worker-node-1
k describe node worker-node-1
k describe node worker-node-1 | less
k get namespaces
k get names
k get namess
k get namesp
k get namespaces
k describe node worker-node-1 | less
k get pods
k get pods -A
k get pods -A -o wide
ls -l /etc/kubernetes
ls -l /etc/kubernetes/manifests/
ls -l /etc/kubernetes/pki
ls -l /etc/kubernetes/admin.conf 
less /etc/kubernetes/admin.conf
sudo less /etc/kubernetes/admin.conf 
k get pods -n kube-system
k get pods -n kube-system -o wide
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml
k get pods 
k get pods -n kube-system 
crictl ps
sudo crictl
which crictl
ls -ltr /usr/bin/crictl 
id crictl
cat /etc/group
k get pods -n kubernetes-dashboard
k get pods -n kubernetes-dashboard -o wide
k get deploy -n kubernetes-dashboard -o wide
k get svc -n kubernetes-dashboard -o wide
k edit svc kubernetes-dashboard kubernetes-dashboard
k edit svc -n kubernetes-dashboard kubernetes-dashboard
k get svc -n kubernetes-dashboard -o wide
k get pods -n kubernetes-dashboard
k get pods -n kubernetes-dashboard -o wide
k get svc -n kubernetes-dashboard -o wide
kubectl get nodes -o wide
ls -l
cd cluster-administration/service-account/
cat ServiceAccount.yaml 
k apply -f ServiceAccount.yaml 
vi test.yaml
k apply -f ClusterRoleBinding.yaml 
k -n kubernetes-dashboard create token admin-user
cd
history
k run nginxpod --image=nginx --port 80
sudo apt install etcd-client
ls
cd vscode/
ls
mkdir etcd
cd etcd
sudo chmod a+rw -R /etc/kubernetes/pki
ls -l /etc/kubernetes/pki
ls -ld /etc/kubernetes/pki
sudo ETCDCTL_API=3 etcdctl snapshot save etcd_backup.db --endpoints https://127.0.0.1:2379 --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key --cacert=/etc/kubernetes/pki/etcd/ca.crt
ls -l
less etcd_backup.db 
sudo ETCDCTL_API=3 etcdctl --write-out=table snapshot status etcd_backup.db --endpoints https://127.0.0.1:2379 --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key --cacert=/etc/kubernetes/pki/etcd/ca.crt
k api-resources
history
k get ns
k api-resources
k explain pod.spec.containers
k explain pod.spec.containers --recursive
cd ../workloads/
k get pods
k get po
k get pods
k get nodes
sudo shutdown now
python3
kubectl get nodes -o wide
cd cluster-administration/service-account/
cd
cd vscode/workloads/
k apply -f pod.yaml 
kubectl apply -f pod.yaml
kubectl apply -f multi-container-pod.yaml 
k get p 
alias k=kubectl 
k get p 
k get po
k -n kubernetes-dashboard create token admin-user
alias k=kubectl
k -n kubernetes-dashboard create token admin-user
top
ip a
ping 192.168.1.3
less /etc/kubernetes/manifests/kube-apiserver.yaml 
sudo less /etc/kubernetes/manifests/kube-apiserver.yaml 
ls
cd vscode/
ls
cd workloads/
ls
k apply -f init-container.yaml 
alias k=kubectl
k apply -f init-container.yaml 
k get pod purple -w
k get pod purple
k describe pod purple
k get pods
k delete pod purple
ls -l /home/abhishek
ls -l /home/abhishek/.kube/
cd /home/abhishek/.kube/
pw
pwd
ls -l
cd
ssh-keygen
ssh-copy-id -i ~/.ssh/id_rsa.pub abhishek@192.168.1.11
ssh-copy-id -i ~/.ssh/id_rsa.pub abhishek@192.168.1.12
cat /etc/hosts
sudo vim /etc/hosts
ssh --help
ssh -help
ssh abhishek@worker-1 "ls -l"
ssh abhishek@worker-1
ssh -t abhishek@worker-2 "top -bn1 | grep Cpu"
ssh abhishek@worker-2
ssh -t abhishek@worker-2 "top -bn1 | grep Cpu"
ssh -t abhishek@worker-2 "top -bn1 | grep Mem"
ssh -t abhishek@worker-2 "sudo shutdown now"
sudo shutdown now
kubectl get nodes
kubectl get pods
ls
cd vscode/
cd k8s-cluster/
ls -l
helm
ls -l
cd vscode/
ls -l
sudo apt install git
git clone https://github.com/abhisran/k8s-cluster.git
ls -l
mv cluster-administration etcd workloads k8s-cluster/
ls -la
cd k8s-cluster/
ls -la
git status
git add .
git status
git commit -m "first-commit"
git config --global user.email "abhisran6@gmail.com"
git config --global user.name "abhisran"
git commit -m "first-commit"
git push origin main
git pull
ls -l
helm version
sudo apt update
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
ls -l
helm version
ls
mkdir helm
cd helm/
ls -l
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm 
helm repo 
helm repo list
helm repo update
helm install prometheus prometheus-community/prometheus
alias k=kubectl
k get pod
k get pod | grep prom
k get n
k get ns
k get pods
k get pod prometheus-server-dd484f8d9-jh79b -w
k getet pod -o wide
k get pod -o wide
history | grep ssh
ssh -t abhishek@worker-1 "top -bn1 | grep Cpu"
ssh -t abhishek@worker-1 "top -bn1 | grep mem"
ssh -t abhishek@worker-1 "top -bn1 | grep Mem"
k get pods
k get logs prometheus-server-dd484f8d9-jh79b
k logs prometheus-server-dd484f8d9-jh79b
k log prometheus-server-dd484f8d9-jh79b
k logs prometheus-server-dd484f8d9-jh79b
k get pods
history
helm
helm uninstall prometheus prometheus-community/prometheus
helm list
helm repo list
k get pods
k get pod
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo add grafana https://grafana.github.io/helm-charts
helm repo update
kubectl create namespace monitoring
helm install prometheus prometheus-community/prometheus --namespace monitoring
kubectl get pods -n monitoring
kubectl get pods -n monitoring -w
kubectl get pods -n monitoring
kubectl describe pod prometheus-alertmanager-0 -n monitoring
kubectl describe nodes
kubectl get pvc -n monitoring
ls -l
cd ..
ls -l
mkdir persistent-volumes
cd persistent-volumes
vim prometheus-pv.yaml
ls -l /mnt
sudo mkdir -p /mnt/data/prometheus-server
sudo mkdir -p /mnt/data/prometheus-alertmanager
ls -l
ls -l /mnt
sudo chmod 777 /mnt/data/prometheus-server
sudo chmod 777 /mnt/data/prometheus-alertmanager
ls -ld /mnt/data/prometheus-server
ls -ld /mnt/data/prometheus-alertmanager
kubectl apply -f prometheus-pv.yaml
k get pod
k get pod -n monitoring
k delete pod prometheus-alertmanager-0 -n monitoring
k get pod -n monitoring
k delete pod -n monitoring prometheus-server-dd484f8d9-wrx7x
k get pod -n monitoring
k get pod -n monitoring -w
ssh abhishek@worker-node-1
ssh abhishek@worker-node-2
k get pod -n monitoring
k delete pod -n monitoring prometheus-server-dd484f8d9-wrx7x
k delete pod -n monitoring prometheus-server-dd484f8d9-4l5v8
k get pod -n monitoring
k delete pod prometheus-alertmanager-0 -n monitoring
k get pod -n monitoring
k get pod -n monitoring -w
k describe pod -n monitoring prometheus-alertmanager-0
cd
sudo apt update
sudo apt install nfs-kernel-server
sudo mkdir -p /mnt/nfs-share
sudo chmod 777 /mnt/nfs-share
sudo nano /etc/exports
sudo exportfs -rav
sudo systemctl restart nfs-kernel-server
sudo systemctl enable nfs-kernel-server
showmount -e
k get pod -n monitoring
cd vscode/k8s-cluster/
ls
cd persistent-volumes/
ls
vim nfs-pv.yaml
k apply -f nfs-pv.yaml 
vim nfs-pvc.yaml
k apply -f nfs-pvc.yaml 
helm upgrade prometheus prometheus-community/prometheus --namespace monitoring   --set server.persistentVolume.existingClaim=prometheus-pvc   --set alertmanager.persistentVolume.existingClaim=alertmanager-pvc
kubectl get pods -n monitoring
kubectl delete pod prometheus-alertmanager-0 -n monitoring
kubectl get pods -n monitoring
k get pv
ls -l
k get pv
kubectl delete pvc storage-prometheus-alertmanager-0 -n default
kubectl delete pvc prometheus-server -n monitoring
kubectl delete pv prometheus-alertmanager-pv
kubectl delete pv prometheus-server-pv
kubectl get pv
sudo rm -rf /mnt/data/prometheus-server
sudo rm -rf /mnt/data/prometheus-alertmanager
ls -l /mnt
ls -l /mnt/nfs-share/
k get pod -n monitoring
k describe pod prometheus-alertmanager-0 -n monitoring
history
helm uninstall prometheus prometheus-community/prometheus --namespace monitoring
k get pod -n monitoring
helm install prometheus prometheus-community/prometheus --namespace monitoring
k get pod -n monitoring
k get pod -n monitoring -w
k get pod -n monitoring
k get pvc
k get pv
k get 
k get -h
k get pv
k delete pv nfs-alertmanager-pv
k get pv
k explain pv
k help pv
k get pv
k get pod -n monitoring
helm uninstall prometheus prometheus-community/prometheus --namespace monitoring
k get pv
k delete pv nfs-prometheus-pv
k explain pvc
ls -l
rm prometheus-pv.yaml 
ls
ls -l
cat nfs-pv.yaml 
cat nfs-pvc.yaml 
k get pv
k describe pv nfs-prometheus-pv
k get pvc
kubectl delete pvc prometheus-pvc -n monitoring
kubectl get pvc -n monitoring
kubectl delete pvc alertmanager-pvc -n monitoring
kubectl get pvc -n monitoring
kubectl delete pvc storage-prometheus-alertmanager-0 -n monitoring
kubectl edit pv nfs-prometheus-pv
k get pv
ls -l
k apply -f nfs-pv.yaml 
k apply -f nfs-pvc.yaml 
k get pv
k get pvc
k get pvc -n monitoring
helm install prometheus prometheus-community/prometheus --namespace monitoring
k get pods -n monitoring
k get pods -n monitoring -w
k describe pod -n monitoring prometheus-server-dd484f8d9-6cg2q
k get pods -n monitoring
k describe pod -n prometheus-alertmanager-0
k describe pod -n monitoring prometheus-alertmanager-0
cat *
kubectl delete pvc prometheus-pvc -n monitoring
kubectl delete pvc alertmanager-pvc -n monitoring
helm install prometheus prometheus-community/prometheus --namespace monitoring
helm uninstall prometheus prometheus-community/prometheus --namespace monitoring
helm install prometheus prometheus-community/prometheus --namespace monitoring
kubectl get pvc -n monitoring
kubectl get pods -n monitoring
kubectl get pod -n monitoring prometheus-server-dd484f8d9-tgvhz
kubectl describe pod -n monitoring prometheus-server-dd484f8d9-tgvhz
kubectl get pvc -n monitoring
kubectl get pv
ls
cat *
ls
cat nfs-pv.yaml 
cat nfs-pvc.yaml 
kubectl get pvc -n monitoring
kubectl get pv
kubectl describe pod -n monitoring prometheus-server-dd484f8d9-tgvhz
ls -ltr /mnt
ls -ltr /mnt/nfs-share/
kubectl delete pvc prometheus-pvc alertmanager-pvc -n monitoring
kubectl get pvc -n monitoring
kubectl delete pvc prometheus-server  -n monitoring
kubectl get pvc -n monitoring
kubectl delete pvc storage-prometheus-alertmanager-0  -n monitoring
ls
vi nfs-pv.yaml 
k get pvc
k get pv
k delete pv nfs-alertmanager-pv
k get pv
k delete pv nfs-prometheus-pv
helm uninstall prometheus prometheus-community/prometheus --namespace monitoring
rm -rf *
ls 
cd /mnt
ls
cd nfs-share/
ls -l
rm -rf *
sudo rm -rf *
ls -l
cd
cd vscode/k8s-cluster/
ls
cd persistent-volumes/
vim nfs-pv.yaml
kubectl apply -f prometheus-nfs-pv.yaml
mv nfs-pv.yaml prometheus-nfs-pv.yaml
kubectl apply -f prometheus-nfs-pv.yaml
vim prometheus-nfs-pvc.yaml
kubectl apply -f prometheus-nfs-pvc.yaml
kubectl create namespace monitoring
cd
cd vscode/k8s-cluster/
ls
mkdir prometheus-setup
cd prometheus-setup
ls -l
vim prometheus-values.yaml
helm install prometheus prometheus-community/prometheus -n monitoring -f prometheus-values.yaml
kubectl get pods -n monitoring
kubectl get pvc -n monitoring
ls
cat prometheus-values.yaml 
helm uninstall prometheus prometheus-community/prometheus -n monitoring -f prometheus-values.yaml
helm uninstall prometheus prometheus-community/prometheus -n monitoring 
kubectl get pods -n monitoring
kubectl get pvc -n monitoring
kubectl get pv -n monitoring
vim prometheus-values.yaml 
kubectl delete pvc storage-prometheus-alertmanager-0 -n monitoring
helm upgrade prometheus prometheus-community/prometheus -n monitoring -f /path/to/prometheus-values.yaml
ls
helm install prometheus prometheus-community/prometheus -n monitoring -f prometheus-values.yaml
kubectl get pods -n monitoring
kubectl get pvc -n monitoring
kubectl get pods -n monitoring
k describe pod prometheus-alertmanager-0
k describe pod prometheus-alertmanager-0 -n monitoring
ca prometheus-values.yaml 
cat prometheus-values.yaml 
vim prometheus-values.yaml 
helm upgrade prometheus prometheus-community/prometheus -n monitoring -f prometheus-values.yaml
kubectl delete pvc storage-prometheus-alertmanager-0 -n monitoring
kubectl get pvc -n monitoring
kubectl get pods -n monitoring
kubectl get pvc -n monitoring
kubectl get pods -n monitoring
sudo shutdown now
ls -l
cd vscode/
ls
cd k8s-cluster/
ls -l
cd ..
rm -rf k8s-cluster/
history | grep git clone
history | grep git
git clone https://github.com/abhisran/k8s-cluster.git
cd k8s-cluster/
ls -l
cd monitoring/
ls -l
k get -n
alias k=kubectl
k get -n
k get n
k get ns
k delete ns monitoring
ssh abhishek@worker-1
k get pv
k get pvc
k delete pv alertmanager-nfs-pv
k delete pv prometheus-nfs-pv
ls -l /mnt/
ls -l /mnt/nfs-share/
cd /mnt/nfs-share/
rm -rf *
sudo rm -rf *
ssh abhishek@worker-1
mkdir -m 777 prometheus
ls -l
mkdir -m 777 grafana
helm list
cd
cd vscode/k8s-cluster/
ls -l
cd monitoring/
k create ns monitoring
ls -l
k apply -f local-storageclass.yaml 
cd volume/
k apply -f pv-config.yaml 
ls -l
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update
helm install prometheus prometheus-community/kube-prometheus-stack -n monitoring -f values.yaml
cd ..
ls -l
helm install prometheus prometheus-community/kube-prometheus-stack -n monitoring -f values.yaml
kubectl get statefulsets -n monitoring
kubectl get pods -n monitoring
kubectl get statefulsets -n monitoring
kubectl get pods -n monitoring
kubectl patch svc prometheus-kube-prometheus-prometheus -n monitoring -p '{"spec": {"type": "NodePort"}}'
kubectl get svc prometheus-kube-prometheus-prometheus -n monitoring
kubectl patch svc prometheus-grafana -n monitoring -p '{"spec": {"type": "NodePort"}}'
kubectl get svc prometheus-grafana -n monitoring
kubectl get nodes -o wide
k get pods -n monitoring -o wide
ls -l /mnt/nfs-share/
ls -l /mnt/nfs-share/grafana/
ls -l /mnt/nfs-share/prometheus/
ls -l /mnt/nfs-share/prometheus/prometheus-db/
kubectl get svc prometheus-grafana -n monitoring
kubectl get svc prometheus-kube-prometheus-prometheus -n monitoring
top -bn1 | grep Cpu
top -bn1 | grep Mem
df -h
kubectl get secret prometheus-grafana -n monitoring -o jsonpath="{.data.admin-password}" | base64 --decode ; echo
k get svc
k get ns
k get svc -n kubernetes-dashboard
history | grep token
k -n kubernetes-dashboard create token admin-user
helm list
helm list -n monitoring
helm uninstall prometheus -n monitoring
top
k get pods -n monitoring
k get svc -n monitoring
k get pvc -n monitoring
k get pv -n monitoring
k delete ns monitoring
sudo shutdown now
exit
ls
cd vscode/
ls
cd k8s-cluster/
git status
git add .
git commit -m "updated storage path"
git pull origin main
git push origin main
alias k=kubectl
cd
k api-resource | grep Job
k api-resources | grep Job
k explain Job
k explain Job.spec
k explain Job.spec.template
k explain Job.spec.template.spec
k explain Job.spec.template.spec.restartPolicy
ip a
k get ns
history
k -n kubernetes-dashboard create token admin-user
k get svc -n kubernetes-dashboard
k explain Job.spec.template.spec.restartPolicy
k explain Job.spec.backofLimit
k explain Job.spec
ls
cd vscode/
ls
cd k8s-cluster/
ls
mkdir exercise-1
cd exercise-1
vim exercise-1.yaml
k apply -f exercise-1.yaml 
k get pod
k get pod | grep kucc8
cat exercise-1.yaml 
k get pod
k get pod | grep kucc8
k logs kucc8
df -h
cd vscode/
cd k8s-cluster/
cd exercise-1/
ls -l
k get pods
alias k=kubectl
k get pods
k delete pod kucc8
vim exercise-2.yaml
k explain Deploy
k explain Deployment
k explain Deployment.spec
vim exercise-2.yaml
k explain Deployment.spec
alias k=kubectl
k explain Deployment.spec
k explain Deployment.spec.selector
k explain Deployment.template
k explain Deployment
k explain Deployment.spec.template
k explain Deployment.spec
k explain Deployment.spec.strategy
k explain Deployment.spec.selector
k explain Deployment.spec.template
k explain Deployment.spec.template.metadata
k explain Deployment.spec.template.spec
k explain Deployment.spec
k explain Deployment.spec.strategy
k explain Deployment
alias k=kubectl
vim exercise-2.yaml
cd vscode/k8s-cluster/exercise-1/
vim exercise-2.yaml
cat exercise-
cat exercise-1.yaml 
vim exercise-2.yaml
k apply -f exercise-2.yaml 
vim exercise-2.yaml
cat exercise-2.yaml 
vim exercise-2.yaml
k apply -f exercise-2.yaml 
vim exercise-2.yaml
k apply -f exercise-2.yaml 
cat exercise-2.yaml 
vim exercise-2.yaml
k apply -f exercise-2.yaml 
vim exercise-2.yaml
k apply -f exercise-2.yaml 
cat exercise-
cat exercise-2.yaml 
vim exercise-2.yaml
k apply -f exercise-2.yaml 
k get deploy
k get deploy -o wide
k get deploy
k describe deloy recreate-deploy
k describe deploy recreate-deploy
history | grep token
kubectl -n kubernetes-dashboard create token admin-user
kubectl explain pod
kubectl explain metadata
kubectl explain pod.metadata
kubectl explain pod.spec
kubectl explain pod.spec.affinity
kubectl explain pod.spec.affinity.nodeAffinity
history | grep alias
alias k=kubectl
k explain Deployment.spec.strategy
k explain pod.spec.affinit
k explain pod.spec.affinity
k explain pod.spec.affinity.nodeAffinity
k explain pod.spec.affinity.nodeAffinity.preferredDuringSchedulingIgnoredDuringExecution
k explain pod.spec.affinity.nodeAffinity.preferredDuringSchedulingIgnoredDuringExecution.weight
k explain labels
k explain label
k get node
k label node worker-node-1 disktype=ssd
k get node worker-node-1
k get node worker-node-1 -o wide
k get -l node worker-node-1
k get node worker-node-1 --show-label
k get node worker-node-1 --show-labels
k get node --show-labels
cd vscode/k8s-cluster/
mkdir scheduler
cd scheduler
vim node-name-pod.yaml
k apply -f node-name-pod.yaml 
k get pod/node-name-pod
k describe pod/node-name-pod
k get pod/node-name-pod -o wide
k get node --show-labels
k get node worker-node-1 --show-labels
k get nodes -l disktype=ssd
k get nodes -l disktype=ssdf
vim node-selector.yaml 
k apply -f node-selector.yaml 
k get pod/node-selector -o wide
k get pod -o wide | grep worker-node-1
k label node worker-node-2 disktype=ssd
k label node worker-node-2 network=fast
k get node worker-node-2 --show-labels
vim aff-pod.yaml
k apply -f aff-pod.yaml 
k get pod/affinity-pod -o wide
k get nodes -l network=fast
k get nodes -l disktype=ssd
history | grep token
kubectl -n kubernetes-dashboard create token admin-user
cd vscode/k8s-cluster/scheduler/
ls -l
mv node-name-pod.yaml nodeName.yaml
vi nodeName.yaml 
k apply -f nodeName.yaml 
alias k=kubectl
k apply -f nodeName.yaml 
k get node
k get pods
k describe pod/node-name-pod
cat nodeName.yaml 
k describe pod node-name-pod
k delete pod node-name-pod
k apply -f nodeName.yaml 
k describe pod node-name-pod
k delete pod node-name-pod
k get pods --show-labels
k get pods -l app=nginx
k get deploy
k delete deploy recreate-deploy
k get pods
k describe pod node-selector
ls
cat node-selector.yaml 
k get pods
k delete pod nginx
k delete node-selector.yaml
k delete -f node-selector.yaml
top
w
who
k get node --show-labels
history | grep fast
k label node worker-node-1 network=fast
k get node -l network=fast
ls
k get pod
k get pods
cat aff-pod.yaml 
cd ..
ls
cd workloads/
ls
cat daemonset.yaml 
docker ps
sudo crictl ps
k apply -f daemonset.yaml 
k get ds
k get ds -n kube-system
k delete -f daemonset.yaml 
vim daemonset.yaml 
k apply -f daemonset.yaml 
k get ds -n kube-system
k get pods -o wide -n kube-system grep fluentd-elasticsearch
k get pods -o wide -n kube-system | grep fluentd-elasticsearch
k delete -f daemonset.yaml 
cd ..
ls -l
history >> history.text 
less history.text 
history > history.text 